{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEEP NEURAL NETWORK "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "sns.set_theme(style='darkgrid', palette='hls')\n",
    "\n",
    "import torch \n",
    "from torchvision.transforms import ToTensor\n",
    "from torch import nn \n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader\n",
    "from collections import OrderedDict\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tabular data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_Custom(Dataset):\n",
    "    def __init__(self, X_data, Y_data):\n",
    "        X_data = X_data\n",
    "        Y_data = Y_data\n",
    "        self.x, self.y = self.clean(X_data, Y_data)\n",
    "        self.samples = self.y.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx] \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.samples\n",
    "\n",
    "    def clean(self,X_data,Y_data):\n",
    "        scaler = StandardScaler()\n",
    "        data_x = scaler.fit_transform(X_data)\n",
    "        x = torch.from_numpy(data_x).float()\n",
    "        y = torch.from_numpy(Y_data.values).float()\n",
    "        y = y[:,None]\n",
    "        return x,y\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separating into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train (5482, 99) y_train (5482,) \n",
      "X_test (1371, 99) y_test (1371,)\n"
     ]
    }
   ],
   "source": [
    "file = './data_processed/embeddings_spacy.csv'\n",
    "data = pd.read_csv(file, index_col=0)\n",
    "\n",
    "X = data.iloc[:,:-1]\n",
    "y = data[data.columns[-1]]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=20, shuffle=True)\n",
    "\n",
    "print(\"X_train {} y_train {} \\nX_test {} y_test {}\".format(X_train.shape, y_train.shape, X_test.shape, y_test.shape))\n",
    "\n",
    "dataset_train = Dataset_Custom(X_train, y_train)\n",
    "dataset_test = Dataset_Custom(X_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using DataLoaders for both sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 30\n",
    "\n",
    "trainloader = DataLoader(dataset=dataset_train, batch_size=batch, shuffle=True)\n",
    "testloader = DataLoader(dataset=dataset_test, batch_size=batch, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_size, hidden_sizes, output_size, activation_functions):\n",
    "    layers = []\n",
    "    layer_sizes = [input_size] + hidden_sizes + [output_size]\n",
    "    \n",
    "    # Add hidden layers\n",
    "    for i in range(1, len(layer_sizes)):\n",
    "        layers.append(('fc{}'.format(i), nn.Linear(layer_sizes[i-1], layer_sizes[i])))\n",
    "        if activation_functions[i-1] == 'relu':\n",
    "            layers.append(('relu{}'.format(i), nn.ReLU()))\n",
    "        elif activation_functions[i-1] == 'sigmoid':\n",
    "            layers.append(('sigmoid{}'.format(i), nn.Sigmoid()))\n",
    "\n",
    "    model = nn.Sequential(OrderedDict(layers))\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of loss, learning rate & epochs\n",
    "def train_rna(model):\n",
    "    learning_rate = 0.0001\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "    epochs = 100\n",
    "    #print_every = int((X_train.shape[0]/batch) / 20)\n",
    "    #steps = 0 \n",
    "    #list_loss = []\n",
    "    #list_loss_test = []\n",
    "    #list_epochs = []\n",
    "\n",
    "    # Comenzando entrenamiento. . . \n",
    "    for e in range(epochs):\n",
    "        #running_loss = 0\n",
    "        # En cada epoca cargamos todos los batches \n",
    "        for inputs, labels in iter(trainloader):\n",
    "            #steps += 1\n",
    "            # Reiniciar los gradientes\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            outputs = model.forward(inputs)\n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            # Backprogation\n",
    "            loss.backward()\n",
    "            # Actualiza los pesos de acuerdo a un paso del optimizador\n",
    "            optimizer.step()\n",
    "            # Guardamos la perdida para control del entrenamiento\n",
    "            #running_loss += loss.item()\n",
    "            # imprimimos cada 20% lotes\n",
    "            #if steps % print_every == 0:\n",
    "            #    list_loss.append(running_loss/print_every)\n",
    "            #    running_loss = 0\n",
    "                \n",
    "            #    output_test = model.forward(dataset_test.x)\n",
    "            #    loss_t = criterion(output_test, dataset_test.y)\n",
    "            #    list_loss_test.append(loss_t.item())\n",
    "            #    list_epochs.append(e+1)\n",
    "            \n",
    "    #df = pd.DataFrame()\n",
    "    #df['epochs'] = list_epochs\n",
    "    #df['loss_train'] = list_loss\n",
    "    #df['loss_test'] = list_loss_test\n",
    "\n",
    "    #sns.set_theme(style=\"darkgrid\")\n",
    "\n",
    "    #sns.lineplot(x=\"epochs\", y=\"loss_train\", data=df, color='yellow', label='Entrenamiento')\n",
    "    #sns.lineplot(x=\"epochs\", y=\"loss_test\", data=df, color='violet', label='Prueba')\n",
    "\n",
    "    output_acc = model.forward(dataset_test.x)\n",
    "\n",
    "    # Medidas\n",
    "    pred = torch.unsqueeze(torch.tensor([ 0 if p<=0.5 else 1 for p in output_acc]),1)\n",
    "    label = dataset_test.y\n",
    "    pred = pred.numpy()\n",
    "    label = label.numpy()\n",
    "\n",
    "    # Calcular la exactitud\n",
    "    exactitud = round(accuracy_score(label, pred),2)\n",
    "    #print(\"Exactitud:\", exactitud)\n",
    "\n",
    "    # Calcular la preci sión\n",
    "    precision = round(precision_score(label, pred),2)\n",
    "    #print(\"Precisión:\", precision)\n",
    "\n",
    "    # Calcular la puntuación F1\n",
    "    f1 = round(f1_score(label, pred),2)\n",
    "    #print(\"Puntuación F1:\", f1)\n",
    "\n",
    "    mean_scores = round(exactitud ,4)\n",
    "    return mean_scores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmo genético para optimizar la estructura de la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generacion  0\n",
      "Mejor aptitud para esta generación:  0.56\n",
      "{'input_size': 99, 'hidden_sizes': [40], 'activation': ['relu', 'sigmoid'], 'output_size': 1, 'fit': 0.56}\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 124\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m paro:\n\u001b[1;32m    123\u001b[0m     individuos \u001b[39m=\u001b[39m selection(individuos) \u001b[39m# Seleccionamos los mejores\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m     individuos \u001b[39m=\u001b[39m CrossoverAndMutation(individuos)\n\u001b[1;32m    125\u001b[0m     networks \u001b[39m=\u001b[39m generateNetworks(individuos) \n\u001b[1;32m    126\u001b[0m     individuos \u001b[39m=\u001b[39m fitness(networks, individuos)\n",
      "Cell \u001b[0;32mIn[11], line 84\u001b[0m, in \u001b[0;36mCrossoverAndMutation\u001b[0;34m(individuos)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[39mfor\u001b[39;00m mutation \u001b[39min\u001b[39;00m mutations:\n\u001b[1;32m     83\u001b[0m     individuo \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39minput_size\u001b[39m\u001b[39m'\u001b[39m: input_size ,\u001b[39m'\u001b[39m\u001b[39mhidden_sizes\u001b[39m\u001b[39m'\u001b[39m:[] , \u001b[39m'\u001b[39m\u001b[39mactivation\u001b[39m\u001b[39m'\u001b[39m:[], \u001b[39m'\u001b[39m\u001b[39moutput_size\u001b[39m\u001b[39m'\u001b[39m : output_size , \u001b[39m'\u001b[39m\u001b[39mfit\u001b[39m\u001b[39m'\u001b[39m : \u001b[39m0.0\u001b[39m}\n\u001b[0;32m---> 84\u001b[0m     individuo[\u001b[39m'\u001b[39m\u001b[39mhidden_sizes\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m individuos[mutation][\u001b[39m'\u001b[39m\u001b[39mhidden_sizes\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m     85\u001b[0m     individuo[\u001b[39m'\u001b[39m\u001b[39mactivation\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m individuos[mutation][\u001b[39m'\u001b[39m\u001b[39mactivation\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m     87\u001b[0m     \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m,\u001b[39m2\u001b[39m) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(individuo[\u001b[39m'\u001b[39m\u001b[39mhidden_sizes\u001b[39m\u001b[39m'\u001b[39m]) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Generacion de la poblacion inicial\n",
    "# 5 redes 1 capa oculta , 5 redes 2 capa oculta , 5 redes 3 capa oculta , 5 redes 4 capa oculta\n",
    "# No. Neuronas por capa (3, 50)\n",
    "no_individuos = 40\n",
    "input_size = X.shape[1]\n",
    "output_size = 1\n",
    "\n",
    "\n",
    "individuos = []\n",
    "# -------------------------- Generacion de la poblacion inicial -----------------------------------------\n",
    "\n",
    "# Generamos las caracteristicas de los individuos\n",
    "for i in range(4):\n",
    "    # Redes\n",
    "    for j in range(10):\n",
    "        individuo = {'input_size': input_size ,'hidden_sizes':[] , 'activation':[], 'output_size' : output_size , 'fit' : 0.0}\n",
    "        for k in range(i+1):\n",
    "            individuo['hidden_sizes'].append(np.random.randint(3,51))\n",
    "            individuo['activation'].append('relu')\n",
    "        individuo['activation'].append('sigmoid')\n",
    "        individuos.append(individuo)\n",
    "\n",
    "# -------------------------- Funcion para generar los modelos a partir de sus caracteristicas ---------------\n",
    "\n",
    "# Generamos los individuos en base a las caracteristicas\n",
    "def generateNetworks(individuos):\n",
    "    networks = []\n",
    "    for item in individuos:\n",
    "        networks.append(build_model(item['input_size'], item['hidden_sizes'] , item['output_size'], item['activation']))\n",
    "    return networks\n",
    "\n",
    "# -------------------------- Evaluacion del modelo -------------------\n",
    "\n",
    "# Evaluation\n",
    "def fitness(networks:list, individuos):\n",
    "    for no in range(no_individuos):\n",
    "        individuos[no]['fit'] = train_rna(networks[no])  \n",
    "    return individuos\n",
    "\n",
    "# -------------------------- Seleccion (7 mejores y 3 peores) -------------------\n",
    "\n",
    "# Selection\n",
    "def selection(individuos):\n",
    "    individuos = sorted(individuos, key=lambda x: x['fit'], reverse=True)\n",
    "    # Se seleccionan los mejores 7 y los ultimos 3 \n",
    "    individuos = individuos[0:14] + individuos[-6:]\n",
    "    return individuos\n",
    "\n",
    "# -------------------------- Cruza y mutacion -------------------\n",
    "\n",
    "def CrossoverAndMutation(individuos):\n",
    "    # Aleatorizamos los individuos seleccionados para agregar diversidad y evitar elitismo\n",
    "    # Cross-over and mutation 5 new -> cross over 5 -> mutation\n",
    "    random.shuffle(individuos)\n",
    "\n",
    "    # Cross-over\n",
    "    for i in range(0,len(individuos),2):\n",
    "        individuo = {'input_size': input_size ,'hidden_sizes':[] , 'activation':[], 'output_size' : output_size , 'fit' : 0.0}\n",
    "        # Aleatoria un numero para saber que capas va a heredar\n",
    "        if np.random.randint(0,2) == 0:\n",
    "            individuo['hidden_sizes'] = individuos[i]['hidden_sizes'].copy()\n",
    "            individuo['activation'] = individuos[i]['activation'].copy()\n",
    "            individuo['hidden_sizes'][np.random.randint(0,len(individuo['hidden_sizes']))] = individuos[i+1]['hidden_sizes'][np.random.randint(0,len(individuos[i+1]['hidden_sizes']))]\n",
    "        else :\n",
    "            individuo['hidden_sizes'] = individuos[i+1]['hidden_sizes'].copy()\n",
    "            individuo['activation'] = individuos[i+1]['activation'].copy()\n",
    "            individuo['hidden_sizes'][np.random.randint(0,len(individuo['hidden_sizes']))] = individuos[i]['hidden_sizes'][np.random.randint(0,len(individuos[i]['hidden_sizes']))]\n",
    "        \n",
    "        individuos.append(individuo)\n",
    "        \n",
    "    # Mutations\n",
    "    mutations = []\n",
    "    while len(mutations) != no_individuos/4:\n",
    "        index = np.random.randint(0,no_individuos)\n",
    "        if len(mutations) == 0:\n",
    "            mutations.append(index)\n",
    "        else:\n",
    "            if index not in mutations:\n",
    "                mutations.append(index)\n",
    "    # Copiar un gen == 1 \n",
    "    # Crear nuevo gen == 0\n",
    "    for mutation in mutations:\n",
    "        individuo = {'input_size': input_size ,'hidden_sizes':[] , 'activation':[], 'output_size' : output_size , 'fit' : 0.0}\n",
    "        individuo['hidden_sizes'] = individuos[mutation]['hidden_sizes'].copy()\n",
    "        individuo['activation'] = individuos[mutation]['activation'].copy()\n",
    "\n",
    "        if np.random.randint(0,2) == 0 or len(individuo['hidden_sizes']) == 1:\n",
    "            individuo['hidden_sizes'][np.random.randint(0,len(individuo['hidden_sizes']))] = np.random.randint(3,51)\n",
    "        else:\n",
    "            id_ = np.random.randint(0,len(individuo['hidden_sizes']))\n",
    "            id_new = np.random.randint(0,len(individuo['hidden_sizes']))\n",
    "            while id_ == id_new:\n",
    "                id_new = np.random.randint(0,len(individuo['hidden_sizes']))\n",
    "            individuo['hidden_sizes'][id_] = individuo['hidden_sizes'][id_new]\n",
    "\n",
    "        individuos.append(individuo)\n",
    "        \n",
    "    return individuos\n",
    "\n",
    "def criteria(individuos):\n",
    "    individuos = sorted(individuos, key=lambda x: x['fit'], reverse=True)\n",
    "    print('Mejor aptitud para esta generación: ', individuos[0]['fit'])\n",
    "    print(individuos[0])\n",
    "    if individuos[0]['fit'] >= 0.65:\n",
    "        file = open('./GeneticsResults.txt', mode='w',encoding='utf-8')\n",
    "        file.write('Arquitectura encontrada!\\n')\n",
    "        file.write(str(individuos[0]))\n",
    "        file.close()\n",
    "        return True\n",
    "    else: \n",
    "        return False\n",
    "\n",
    "# -------------------------- ALGORITMO GENETICO -------------------\n",
    "\n",
    "# Evaluar la poblacion inicial\n",
    "\n",
    "noGeneracion = 0\n",
    "networks = generateNetworks(individuos) # Generar modelos usando caracteristicas\n",
    "individuos = fitness(networks, individuos) # Evaluamos cada modelo\n",
    "print('Generacion ',noGeneracion)\n",
    "paro = criteria(individuos)\n",
    "while not paro:\n",
    "    individuos = selection(individuos) # Seleccionamos los mejores\n",
    "    individuos = CrossoverAndMutation(individuos)\n",
    "    networks = generateNetworks(individuos) \n",
    "    individuos = fitness(networks, individuos)\n",
    "    noGeneracion += 1\n",
    "    print('Generacion ',noGeneracion)\n",
    "    paro = criteria(individuos)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
